{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 10:37:20.664831: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 10:37:20.724566: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-07 10:37:21.040746: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 10:37:21.040780: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 10:37:21.040784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import compress_pickle as pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GlobalAvgPool\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Disable GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting the model hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Config\n",
    "################################################################################\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "epochs = 400  # Number of training epochs\n",
    "es_patience = 10  # Patience for early stopping\n",
    "batch_size = 32  # Batch size\n",
    "seed = 42  # Fixing random seed for reproducibility\n",
    "################################################################################"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset in spektral data format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Don't change this\n",
    "remake_dataset = True\n",
    "\n",
    "# Select the dataset to use out of the following: p, ds, cs\n",
    "dataset_id = \"p\"\n",
    "data_dir = Path(f'../data/communication_networks/{dataset_id}')\n",
    "\n",
    "# Load the datasets\n",
    "users_df = pd.read_csv(data_dir / 'metadata/users.csv')\n",
    "questions_df = pd.read_csv(data_dir / 'metadata/questions.csv')\n",
    "answers_df = pd.read_csv(data_dir / 'metadata/answers.csv')\n",
    "comments_df = pd.read_csv(data_dir / 'metadata/comments.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.48; 1: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Load the embeddings of the nodes\n",
    "def get_embeddings(node):\n",
    "    node_type = [0.0, 0.0, 0.0, 0.0]\n",
    "    if node[0:2] == 'q_':\n",
    "        embd = questions_df[questions_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[0] = 1.0\n",
    "    elif node[0:2] == 'a_':\n",
    "        embd = answers_df[answers_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[1] = 1.0\n",
    "    elif node[0:2] == 'c_':\n",
    "        embd = comments_df[comments_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[2] = 1.0\n",
    "    elif node[0:2] == 'u_':\n",
    "        embd = users_df[users_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[3] = 1.0\n",
    "\n",
    "        # Some users have no records in the users table\n",
    "        # Use the embeddings of '' as a placeholder\n",
    "        if len(embd) == 0:\n",
    "            embd = users_df[users_df['Id'] == 8]['embeddings'].values\n",
    "\n",
    "    # Change the following line to use different node representations\n",
    "    #return np.concatenate((json.loads(embd[0]), node_type))\n",
    "    #return json.loads(embd[0])\n",
    "    return node_type\n",
    "\n",
    "\n",
    "# Create a custom dataset for spektral\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "\n",
    "        def make_graphs():\n",
    "            graphs = []\n",
    "            labels = []\n",
    "\n",
    "            for counter, graph_fname in enumerate((data_dir / 'graphs').glob('*.csv')):\n",
    "\n",
    "                try:\n",
    "                    nodes = set()\n",
    "                    edges = set()\n",
    "                    q_id = int(graph_fname.parts[-1].split('.')[0][1:])\n",
    "\n",
    "                    if math.isnan(questions_df[questions_df['Id'] == q_id]['AcceptedAnswerId'].values[0]):\n",
    "                        label = [1, 0]\n",
    "                        labels.append(0)\n",
    "                    else:\n",
    "                        labels.append(1)\n",
    "                        label = [0, 1]\n",
    "\n",
    "                    with io.open(graph_fname, 'r') as f:\n",
    "                        for line in f:\n",
    "                            a, b = line.strip().split(',')\n",
    "                            nodes.add(a)\n",
    "                            nodes.add(b)\n",
    "                            edges.add((a, b))\n",
    "\n",
    "                    nodes = list(nodes)\n",
    "                    encoded_nodes = dict()\n",
    "                    for i in range(len(nodes)):\n",
    "                        encoded_nodes[nodes[i]] = i\n",
    "\n",
    "                    encoded_edges = []\n",
    "                    for e in edges:\n",
    "                        encoded_edges.append((encoded_nodes[e[0]], encoded_nodes[e[1]]))\n",
    "\n",
    "                    node_features = []\n",
    "                    for node in nodes:\n",
    "                        node_features.append(get_embeddings(node))\n",
    "\n",
    "                    node_features = np.array(node_features)\n",
    "\n",
    "                    nodes = [encoded_nodes[n] for n in nodes]\n",
    "\n",
    "                    G = nx.Graph()\n",
    "                    G.add_nodes_from(nodes)\n",
    "                    G.add_edges_from(edges)\n",
    "\n",
    "                    spektral_graph = Graph(x=node_features, a=nx.adjacency_matrix(G, nodelist=nodes), y=label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    # print(traceback.format_exc())\n",
    "                    continue\n",
    "                else:\n",
    "                    graphs.append(spektral_graph)\n",
    "                finally:\n",
    "                    if counter > 1000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "            p1labels = sum(labels) / len(labels)\n",
    "            p0labels = 1 - p1labels\n",
    "\n",
    "            print(f\"0: {p0labels:.2f}; 1: {p1labels:.2f}\")\n",
    "\n",
    "            return graphs\n",
    "\n",
    "        return make_graphs()\n",
    "\n",
    "\n",
    "# If the remake_dataset flag is set to True, the dataset is created from scratch and saved in a pickle file otherwise it is loaded from the pickle file\n",
    "if remake_dataset:\n",
    "    data = CustomDataset(transforms=NormalizeAdj())\n",
    "    with io.open(data_dir / 'spektral/data.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "else:\n",
    "    with io.open(data_dir / 'spektral/data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Convert the one-hot encoded labels to integers\n",
    "determine_label = lambda x: 0 if x == [1, 0] else 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Build a graph convolutional neural network model\n",
    "################################################################################\n",
    "class Net(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCSConv(32, activation=\"relu\")\n",
    "        self.conv2 = GCSConv(32, activation=\"relu\")\n",
    "        self.conv3 = GCSConv(32, activation=\"relu\")\n",
    "        self.global_pool = GlobalAvgPool()\n",
    "        self.dense = Dense(data.n_labels, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.conv3([x, a])\n",
    "        output = self.global_pool([x, i])\n",
    "        output = self.dense(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(loader):\n",
    "    y_real = []\n",
    "    y_hat = []\n",
    "\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "\n",
    "        y_real += [determine_label(list(i)) for i in target]\n",
    "        y_hat += np.argmax(pred.numpy(), axis=1).tolist()\n",
    "\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1]), (\n",
    "                recall_score(y_real, y_hat), precision_score(y_real, y_hat), f1_score(y_real, y_hat))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: full; 0: 0.48; 1: 0.52\n",
      "Fold 0:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.48; 1: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 10:38:13.157176: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-07 10:38:13.157192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: bfg5\n",
      "2023-02-07 10:38:13.157196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: bfg5\n",
      "2023-02-07 10:38:13.157257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.108.3\n",
      "2023-02-07 10:38:13.157269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.108.3\n",
      "2023-02-07 10:38:13.157272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.108.3\n",
      "2023-02-07 10:38:13.157432: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping after 17 epochs with best val_acc: 0.580; best val_rec: 0.681; best val_prec: 0.579; best val_f1: 0.626\n",
      "========================================================================================================================================================================================================\n",
      "Fold 1:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.49; 1: 0.51\n",
      "Early stopping after 21 epochs with best val_acc: 0.571; best val_rec: 0.663; best val_prec: 0.572; best val_f1: 0.614\n",
      "========================================================================================================================================================================================================\n",
      "Fold 2:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.49; 1: 0.51\n",
      "Early stopping after 17 epochs with best val_acc: 0.570; best val_rec: 0.602; best val_prec: 0.580; best val_f1: 0.591\n",
      "========================================================================================================================================================================================================\n",
      "Fold 3:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.48; 1: 0.52\n",
      "Early stopping after 24 epochs with best val_acc: 0.564; best val_rec: 0.658; best val_prec: 0.566; best val_f1: 0.609\n",
      "========================================================================================================================================================================================================\n",
      "Fold 4:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.48; 1: 0.52\n",
      "Early stopping after 31 epochs with best val_acc: 0.589; best val_rec: 0.719; best val_prec: 0.582; best val_f1: 0.643\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the X and y arrays\n",
    "X = data\n",
    "\n",
    "y = [determine_label(i.y) for i in data]\n",
    "\n",
    "\n",
    "# Print the class ratio\n",
    "def print_class_ratio(y, ds_name=\"full\"):\n",
    "    l1 = sum(y) / len(y)\n",
    "    l0 = 1 - l1\n",
    "    print(f\"Dataset: {ds_name}; 0: {l0:.2f}; 1: {l1:.2f}\")\n",
    "\n",
    "\n",
    "print_class_ratio(y)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Use stratified k-fold cross validation to evaluate the model on the dataset\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    y_train = [determine_label(i.y) for i in data[train_index]]\n",
    "    print_class_ratio(y_train, ds_name=\"train\")\n",
    "\n",
    "    y_val = [determine_label(i.y) for i in data[val_index]]\n",
    "    print_class_ratio(y_val, ds_name=\"val\")\n",
    "\n",
    "    loader_tr = DisjointLoader(data[train_index], batch_size=batch_size, epochs=epochs)\n",
    "    loader_va = DisjointLoader(data[val_index], batch_size=batch_size)\n",
    "\n",
    "    model = Net()\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Fit model on training set\n",
    "    ################################################################################\n",
    "    @tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "    def train_step(inputs, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "        return loss, acc\n",
    "\n",
    "\n",
    "    epoch = step = 0\n",
    "    best_val_loss = np.inf\n",
    "    best_weights = None\n",
    "    patience = es_patience\n",
    "    for batch in loader_tr:\n",
    "        step += 1\n",
    "\n",
    "        train_step(*batch)\n",
    "\n",
    "        if step == loader_tr.steps_per_epoch:\n",
    "            step = 0\n",
    "            epoch += 1\n",
    "\n",
    "            # Compute validation loss and accuracy\n",
    "            loss_acc, other_metrics = evaluate(loader_va)\n",
    "\n",
    "            val_loss, val_acc = loss_acc\n",
    "            val_rec, val_prec, val_f1 = other_metrics\n",
    "\n",
    "            # Check if loss improved for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_val_acc = val_acc\n",
    "                best_val_rec = val_rec\n",
    "                best_val_prec = val_prec\n",
    "                best_val_f1 = val_f1\n",
    "                patience = es_patience\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print(f\"Early stopping after {epoch} epochs with best val_acc: {best_val_acc:.3f};\"\n",
    "                          f\" best val_rec: {best_val_rec:.3f}; best val_prec: {best_val_prec:.3f}; best val_f1: {best_val_f1:.3f}\")\n",
    "                    results.append((best_val_acc, best_val_rec, best_val_prec, best_val_f1))\n",
    "                    break\n",
    "\n",
    "    print(100 * \"==\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the experiment results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  dataset       acc       rec      prec        f1 method  fold\n0       p  0.579924  0.680851  0.578581  0.625564    GCN     1\n1       p  0.571067  0.663391  0.572034  0.614334    GCN     2\n2       p  0.570224  0.601966  0.579653  0.590599    GCN     3\n3       p  0.564135  0.657658  0.566291  0.608564    GCN     4\n4       p  0.589451  0.719083  0.582228  0.643459    GCN     5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>acc</th>\n      <th>rec</th>\n      <th>prec</th>\n      <th>f1</th>\n      <th>method</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>p</td>\n      <td>0.579924</td>\n      <td>0.680851</td>\n      <td>0.578581</td>\n      <td>0.625564</td>\n      <td>GCN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>p</td>\n      <td>0.571067</td>\n      <td>0.663391</td>\n      <td>0.572034</td>\n      <td>0.614334</td>\n      <td>GCN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>p</td>\n      <td>0.570224</td>\n      <td>0.601966</td>\n      <td>0.579653</td>\n      <td>0.590599</td>\n      <td>GCN</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>p</td>\n      <td>0.564135</td>\n      <td>0.657658</td>\n      <td>0.566291</td>\n      <td>0.608564</td>\n      <td>GCN</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>p</td>\n      <td>0.589451</td>\n      <td>0.719083</td>\n      <td>0.582228</td>\n      <td>0.643459</td>\n      <td>GCN</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the results in a dataframe\n",
    "results_dic = {\"dataset\": dataset_id, \"acc\": [i[0] for i in results], \"rec\": [i[1] for i in results],\n",
    "               \"prec\": [i[2] for i in results], \"f1\": [i[3] for i in results], \"method\": \"GCN\",\n",
    "               \"fold\": [i for i in range(1, 6)]}\n",
    "\n",
    "results_df = pd.DataFrame(results_dic)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Save the results into the clipboard\n",
    "results_df.transpose().to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
