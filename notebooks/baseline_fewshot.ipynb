{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 10:12:33.135893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 10:12:33.205808: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-07 10:12:33.544938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 10:12:33.544989: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 10:12:33.544993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import compress_pickle as pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset as HFDataset\n",
    "from datasets import load_metric\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitTrainer, SetFitModel\n",
    "from setfit import sample_dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from spektral.data import Graph, Dataset\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting the model hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Select the dataset to use out of the following: p, ds, cs\n",
    "dataset_id = \"cs\"\n",
    "data_dir = Path(f'../data/communication_networks/{dataset_id}')\n",
    "\n",
    "# Don't change this\n",
    "remake_dataset = True\n",
    "\n",
    "# Maximum sequence length for the sentence transformer model\n",
    "max_length = 256\n",
    "\n",
    "# Number of samples per class for the few-shot learning\n",
    "num_shots = 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset in spektral data format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "users_df = pd.read_csv(data_dir / 'metadata/users.csv')\n",
    "questions_df = pd.read_csv(data_dir / 'metadata/questions.csv')\n",
    "answers_df = pd.read_csv(data_dir / 'metadata/answers.csv')\n",
    "comments_df = pd.read_csv(data_dir / 'metadata/comments.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.54; 1: 0.46\n"
     ]
    }
   ],
   "source": [
    "# Load the embeddings of the nodes\n",
    "def get_embeddings(node):\n",
    "    node_type = [0.0, 0.0, 0.0, 0.0]\n",
    "    if node[0:2] == 'q_':\n",
    "        embd = questions_df[questions_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[0] = 1.0\n",
    "    elif node[0:2] == 'a_':\n",
    "        embd = answers_df[answers_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[1] = 1.0\n",
    "    elif node[0:2] == 'c_':\n",
    "        embd = comments_df[comments_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[2] = 1.0\n",
    "    elif node[0:2] == 'u_':\n",
    "        embd = users_df[users_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[3] = 1.0\n",
    "\n",
    "        # Some users have no records in the users table\n",
    "        # Use the embeddings of '' as a placeholder\n",
    "        if len(embd) == 0:\n",
    "            embd = users_df[users_df['Id'] == 8]['embeddings'].values\n",
    "\n",
    "    return json.loads(embd[0])\n",
    "\n",
    "\n",
    "# Create a custom dataset for spektral\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    q_ids = []\n",
    "\n",
    "    def read(self):\n",
    "\n",
    "        def make_graphs():\n",
    "            graphs = []\n",
    "            labels = []\n",
    "\n",
    "            for counter, graph_fname in enumerate((data_dir / 'graphs').glob('*.csv')):\n",
    "\n",
    "                try:\n",
    "                    nodes = set()\n",
    "                    edges = set()\n",
    "                    q_id = int(graph_fname.parts[-1].split('.')[0][1:])\n",
    "\n",
    "                    self.q_ids.append(q_id)\n",
    "\n",
    "                    if math.isnan(questions_df[questions_df['Id'] == q_id]['AcceptedAnswerId'].values[0]):\n",
    "                        label = [1, 0]\n",
    "                        labels.append(0)\n",
    "                    else:\n",
    "                        labels.append(1)\n",
    "                        label = [0, 1]\n",
    "\n",
    "                    with io.open(graph_fname, 'r') as f:\n",
    "                        for line in f:\n",
    "                            a, b = line.strip().split(',')\n",
    "                            nodes.add(a)\n",
    "                            nodes.add(b)\n",
    "                            edges.add((a, b))\n",
    "\n",
    "                    nodes = list(nodes)\n",
    "                    encoded_nodes = dict()\n",
    "                    for i in range(len(nodes)):\n",
    "                        encoded_nodes[nodes[i]] = i\n",
    "\n",
    "                    encoded_edges = []\n",
    "                    for e in edges:\n",
    "                        encoded_edges.append((encoded_nodes[e[0]], encoded_nodes[e[1]]))\n",
    "\n",
    "                    node_features = []\n",
    "                    for node in nodes:\n",
    "                        node_features.append(get_embeddings(node))\n",
    "\n",
    "                    node_features = np.array(node_features)\n",
    "\n",
    "                    nodes = [encoded_nodes[n] for n in nodes]\n",
    "\n",
    "                    G = nx.Graph()\n",
    "                    G.add_nodes_from(nodes)\n",
    "                    G.add_edges_from(edges)\n",
    "\n",
    "                    spektral_graph = Graph(x=node_features, a=nx.adjacency_matrix(G, nodelist=nodes), y=label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    # print(traceback.format_exc())\n",
    "                    continue\n",
    "                else:\n",
    "                    graphs.append(spektral_graph)\n",
    "                finally:\n",
    "                    if counter > 1000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "            p1labels = sum(labels) / len(labels)\n",
    "            p0labels = 1 - p1labels\n",
    "\n",
    "            print(f\"0: {p0labels:.2f}; 1: {p1labels:.2f}\")\n",
    "\n",
    "            return graphs\n",
    "\n",
    "        return make_graphs()\n",
    "\n",
    "\n",
    "# If the remake_dataset flag is set to True, the dataset is created from scratch and saved in a pickle file otherwise it is loaded from the pickle file\n",
    "if remake_dataset:\n",
    "    data = CustomDataset(transforms=NormalizeAdj())\n",
    "    with io.open(data_dir / f'spektral/data_fshot_{dataset_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "else:\n",
    "    with io.open(data_dir / f'spektral/data_fshot_{dataset_id}.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Compute the evaluation metrics\n",
    "def compute_metrics(y_hat, y_real):\n",
    "    metric0 = load_metric(\"accuracy\")\n",
    "    metric1 = load_metric(\"precision\")\n",
    "    metric2 = load_metric(\"recall\")\n",
    "    metric3 = load_metric(\"f1\")\n",
    "\n",
    "    predictions, labels = y_real, y_hat\n",
    "\n",
    "    return list(metric0.compute(predictions=predictions, references=labels).values())[0], \\\n",
    "        list(metric1.compute(predictions=predictions, references=labels).values())[0], \\\n",
    "        list(metric2.compute(predictions=predictions, references=labels).values())[0], \\\n",
    "        list(metric3.compute(predictions=predictions, references=labels).values())[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39794 39794\n",
      "Dataset: full; 0: 0.54; 1: 0.46\n",
      "Fold 0:\n",
      "Dataset: train; 0: 0.54; 1: 0.46\n",
      "Dataset: val; 0: 0.54; 1: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "990fed230a6b4287a080d3ea11a987d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5932655657b2419d92f66f3dc9cef5c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 100\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5571350167a4b37a2c49eac71ed3faa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f970abbf0fb441b0ad4d5baeab7559ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4900113079532605, 0.5606476399560922, 0.4538991335258831, 0.5016574585635359)\n",
      "Accuracy on validation: 0.4900113079532605\n",
      "========================================================================================================================================================================================================\n",
      "Fold 1:\n",
      "Dataset: train; 0: 0.54; 1: 0.46\n",
      "Dataset: val; 0: 0.54; 1: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d9dea761e544a8cb5091b5b92e56d8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bd2bb69697f42a9923a73525762824a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 100\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "962f1870eab94810ae1399f3b73a92a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40c078750bc241ecbb52c1f3722170af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5003141098127906, 0.4777716794731065, 0.4563564875491481, 0.4668186083925459)\n",
      "Accuracy on validation: 0.5003141098127906\n",
      "========================================================================================================================================================================================================\n",
      "Fold 2:\n",
      "Dataset: train; 0: 0.54; 1: 0.46\n",
      "Dataset: val; 0: 0.54; 1: 0.46\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3b381950b644689afaa3926cd588d8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "826bb095d3a2427db65c62f8433b099e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 100\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8381ffba7274804a0458073d518f001"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16a5073a70dc429a8469654ef731f4ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5048372911169745, 0.4964324917672887, 0.46206896551724136, 0.4786347400449795)\n",
      "Accuracy on validation: 0.5048372911169745\n",
      "========================================================================================================================================================================================================\n",
      "Fold 3:\n",
      "Dataset: train; 0: 0.54; 1: 0.46\n",
      "Dataset: val; 0: 0.54; 1: 0.46\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d5aedb77b4a45488f07c01edbf7a580"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec57c3faec404158bc2ed81ea56ca12a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 100\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a75ee865ed7044519006440d6345f26f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fba881cb178541a9bacb61eb2aa5bd26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5059680864430205, 0.5148188803512623, 0.4643564356435644, 0.48828735033836546)\n",
      "Accuracy on validation: 0.5059680864430205\n",
      "========================================================================================================================================================================================================\n",
      "Fold 4:\n",
      "Dataset: train; 0: 0.54; 1: 0.46\n",
      "Dataset: val; 0: 0.54; 1: 0.46\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f54e8aa62664347a9297584eaae1191"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cd7a50d989b4262ae7b47ad28e52e6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 100\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b07b93f32c34ea89b31936f2a2314ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bff2fcce98046a7bc100bfb6fbbb751"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5095501382256848, 0.4260225089212188, 0.46135552913198574, 0.4429855858427287)\n",
      "Accuracy on validation: 0.5095501382256848\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the X and y arrays\n",
    "X = np.array(data.q_ids)\n",
    "\n",
    "determine_label = lambda x: 0 if x == [1, 0] else 1\n",
    "y = [determine_label(i.y) for i in data]\n",
    "\n",
    "print(len(X), len(y))\n",
    "\n",
    "# Print the class ratio\n",
    "def print_class_ratio(y, ds_name=\"full\"):\n",
    "    l1 = sum(y) / len(y)\n",
    "    l0 = 1 - l1\n",
    "    print(f\"Dataset: {ds_name}; 0: {l0:.2f}; 1: {l1:.2f}\")\n",
    "\n",
    "\n",
    "print_class_ratio(y)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Use stratified k-fold cross validation to evaluate the model on the dataset\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    X_train = questions_df[questions_df['Id'].isin(X[train_index])]\n",
    "    X_train[\"text\"] = X_train[\"text1\"].values + \". \" + X_train[\"text2\"].values\n",
    "    X_train[\"label\"] = [determine_label(i.y) for i in data[train_index]]\n",
    "    print_class_ratio(X_train[\"label\"], ds_name=\"train\")\n",
    "\n",
    "    X_val = questions_df[questions_df['Id'].isin(X[val_index])]\n",
    "    X_val[\"text\"] = X_val[\"text1\"].values + \". \" + X_val[\"text2\"].values\n",
    "    X_val[\"label\"] = [determine_label(i.y) for i in data[val_index]]\n",
    "    print_class_ratio(X_val[\"label\"], ds_name=\"val\")\n",
    "\n",
    "    # Try to load the model from the cache first then download it from the HuggingFace\n",
    "    try:\n",
    "        model = SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                            cache_dir=\"/tmp/\", local_files_only=True)\n",
    "    except:\n",
    "        model = SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                            cache_dir=\"/tmp/\", local_files_only=False)\n",
    "\n",
    "    model.max_seq_length = max_length\n",
    "\n",
    "    train_dataset = sample_dataset(HFDataset.from_pandas(X_train[[\"text\", \"label\"]]),\n",
    "                                   label_column=\"label\", num_samples=num_shots, seed=seed)\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = SetFitTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=HFDataset.from_pandas(X_val[[\"text\", \"label\"]]),\n",
    "        loss_class=CosineSimilarityLoss,\n",
    "        #metric=\"accuracy\",\n",
    "        metric=compute_metrics,\n",
    "        batch_size=16,\n",
    "        num_iterations=20,  # The number of text pairs to generate for contrastive learning\n",
    "        num_epochs=1,  # The number of epochs to use for contrastive learning\n",
    "        column_mapping={\"text\": \"text\", \"label\": \"label\"},  # Map dataset columns to text/label expected by trainer\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(metrics)\n",
    "\n",
    "    acc, rec, prec, f1 = metrics\n",
    "\n",
    "    results.append((acc, rec, prec, f1))\n",
    "\n",
    "    print(f\"Accuracy on validation: {acc}\")\n",
    "\n",
    "    print(100 * \"==\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the experiment results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  dataset       acc       rec      prec        f1       method  fold\n0      cs  0.490011  0.560648  0.453899  0.501657  FewShot(20)     1\n1      cs  0.500314  0.477772  0.456356  0.466819  FewShot(20)     2\n2      cs  0.504837  0.496432  0.462069  0.478635  FewShot(20)     3\n3      cs  0.505968  0.514819  0.464356  0.488287  FewShot(20)     4\n4      cs  0.509550  0.426023  0.461356  0.442986  FewShot(20)     5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>acc</th>\n      <th>rec</th>\n      <th>prec</th>\n      <th>f1</th>\n      <th>method</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cs</td>\n      <td>0.490011</td>\n      <td>0.560648</td>\n      <td>0.453899</td>\n      <td>0.501657</td>\n      <td>FewShot(20)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cs</td>\n      <td>0.500314</td>\n      <td>0.477772</td>\n      <td>0.456356</td>\n      <td>0.466819</td>\n      <td>FewShot(20)</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cs</td>\n      <td>0.504837</td>\n      <td>0.496432</td>\n      <td>0.462069</td>\n      <td>0.478635</td>\n      <td>FewShot(20)</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cs</td>\n      <td>0.505968</td>\n      <td>0.514819</td>\n      <td>0.464356</td>\n      <td>0.488287</td>\n      <td>FewShot(20)</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cs</td>\n      <td>0.509550</td>\n      <td>0.426023</td>\n      <td>0.461356</td>\n      <td>0.442986</td>\n      <td>FewShot(20)</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the results in a dataframe\n",
    "results_dic = {\"dataset\": dataset_id, \"acc\": [i[0] for i in results], \"rec\": [i[1] for i in results],\n",
    "               \"prec\": [i[2] for i in results], \"f1\": [i[3] for i in results], \"method\": f\"FewShot({num_shots})\",\n",
    "               \"fold\": [i for i in range(1, 6)]}\n",
    "\n",
    "results_df = pd.DataFrame(results_dic)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Save the results into the clipboard\n",
    "results_df.transpose().to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
