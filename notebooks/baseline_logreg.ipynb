{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 10:34:06.436896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 10:34:06.570541: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-07 10:34:07.104990: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 10:34:07.105040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-07 10:34:07.105045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import compress_pickle as pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from spektral.data import Dataset, Graph\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting the model hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset in spektral data format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Don't change this\n",
    "remake_dataset = True\n",
    "\n",
    "# Select the dataset to use out of the following: p, ds, cs\n",
    "dataset_id = \"p\"\n",
    "data_dir = Path(f'../data/communication_networks/{dataset_id}')\n",
    "\n",
    "# Load the datasets\n",
    "users_df = pd.read_csv(data_dir / 'metadata/users.csv')\n",
    "questions_df = pd.read_csv(data_dir / 'metadata/questions.csv')\n",
    "answers_df = pd.read_csv(data_dir / 'metadata/answers.csv')\n",
    "comments_df = pd.read_csv(data_dir / 'metadata/comments.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.48485615456002695; 1: 0.515143845439973\n"
     ]
    }
   ],
   "source": [
    "# Load the embeddings of the nodes\n",
    "def get_embeddings(node):\n",
    "    node_type = [0.0, 0.0, 0.0, 0.0]\n",
    "    if node[0:2] == 'q_':\n",
    "        embd = questions_df[questions_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[0] = 1.0\n",
    "    elif node[0:2] == 'a_':\n",
    "        embd = answers_df[answers_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[1] = 1.0\n",
    "    elif node[0:2] == 'c_':\n",
    "        embd = comments_df[comments_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[2] = 1.0\n",
    "    elif node[0:2] == 'u_':\n",
    "        embd = users_df[users_df['Id'] == int(node[2:])]['embeddings'].values\n",
    "        node_type[3] = 1.0\n",
    "\n",
    "        # Some users have no records in the users table\n",
    "        # Use the embeddings of '' as a placeholder\n",
    "        if len(embd) == 0:\n",
    "            embd = users_df[users_df['Id'] == 8]['embeddings'].values\n",
    "\n",
    "    return json.loads(embd[0])\n",
    "\n",
    "\n",
    "# Create a custom dataset for spektral\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "\n",
    "        def make_graphs():\n",
    "            graphs = []\n",
    "            labels = []\n",
    "\n",
    "            for counter, graph_fname in enumerate((data_dir / 'graphs').glob('*.csv')):\n",
    "\n",
    "                try:\n",
    "                    nodes = set()\n",
    "                    edges = set()\n",
    "                    q_id = int(graph_fname.parts[-1].split('.')[0][1:])\n",
    "\n",
    "                    if math.isnan(questions_df[questions_df['Id'] == q_id]['AcceptedAnswerId'].values[0]):\n",
    "                        label = [1, 0]\n",
    "                        labels.append(0)\n",
    "                    else:\n",
    "                        labels.append(1)\n",
    "                        label = [0, 1]\n",
    "\n",
    "                    with io.open(graph_fname, 'r') as f:\n",
    "                        for line in f:\n",
    "                            a, b = line.strip().split(',')\n",
    "                            nodes.add(a)\n",
    "                            nodes.add(b)\n",
    "                            edges.add((a, b))\n",
    "\n",
    "                    nodes = list(nodes)\n",
    "                    encoded_nodes = dict()\n",
    "                    for i in range(len(nodes)):\n",
    "                        encoded_nodes[nodes[i]] = i\n",
    "\n",
    "                    encoded_edges = []\n",
    "                    for e in edges:\n",
    "                        encoded_edges.append((encoded_nodes[e[0]], encoded_nodes[e[1]]))\n",
    "\n",
    "                    node_features = []\n",
    "                    for node in nodes:\n",
    "                        node_features.append(get_embeddings(node))\n",
    "\n",
    "                    node_features = np.array(node_features)\n",
    "\n",
    "                    nodes = [encoded_nodes[n] for n in nodes]\n",
    "\n",
    "                    G = nx.Graph()\n",
    "                    G.add_nodes_from(nodes)\n",
    "                    G.add_edges_from(edges)\n",
    "\n",
    "                    spektral_graph = Graph(x=node_features, a=nx.adjacency_matrix(G, nodelist=nodes), y=label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    # print(traceback.format_exc())\n",
    "                    continue\n",
    "                else:\n",
    "                    graphs.append(spektral_graph)\n",
    "                finally:\n",
    "                    if counter > 1000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "            p1labels = sum(labels) / len(labels)\n",
    "            p0labels = 1 - p1labels\n",
    "\n",
    "            print(f\"0: {p0labels}; 1: {p1labels}\")\n",
    "\n",
    "            return graphs\n",
    "\n",
    "        return make_graphs()\n",
    "\n",
    "\n",
    "# If the remake_dataset flag is set to True, the dataset is created from scratch and saved in a pickle file otherwise it is loaded from the pickle file\n",
    "if remake_dataset:\n",
    "    data = CustomDataset(transforms=NormalizeAdj())\n",
    "    with io.open(data_dir / 'spektral/data.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "else:\n",
    "    with io.open(data_dir / 'spektral/data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: full; 0: 0.48; 1: 0.52\n",
      "Fold 0:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.48; 1: 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.53      0.52      1149\n",
      "           1       0.54      0.52      0.53      1222\n",
      "\n",
      "    accuracy                           0.52      2371\n",
      "   macro avg       0.52      0.52      0.52      2371\n",
      "weighted avg       0.52      0.52      0.52      2371\n",
      "\n",
      "Accuracy on validation: 0.5221425558835934\n",
      "========================================================================================================================================================================================================\n",
      "Fold 1:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.49; 1: 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.54      0.54      1150\n",
      "           1       0.56      0.56      0.56      1221\n",
      "\n",
      "    accuracy                           0.55      2371\n",
      "   macro avg       0.55      0.55      0.55      2371\n",
      "weighted avg       0.55      0.55      0.55      2371\n",
      "\n",
      "Accuracy on validation: 0.5474483340362716\n",
      "========================================================================================================================================================================================================\n",
      "Fold 2:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.49; 1: 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.51      1150\n",
      "           1       0.54      0.55      0.54      1221\n",
      "\n",
      "    accuracy                           0.53      2371\n",
      "   macro avg       0.53      0.53      0.53      2371\n",
      "weighted avg       0.53      0.53      0.53      2371\n",
      "\n",
      "Accuracy on validation: 0.5301560522986082\n",
      "========================================================================================================================================================================================================\n",
      "Fold 3:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.48; 1: 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.58      0.54      1149\n",
      "           1       0.54      0.46      0.50      1221\n",
      "\n",
      "    accuracy                           0.52      2370\n",
      "   macro avg       0.52      0.52      0.52      2370\n",
      "weighted avg       0.52      0.52      0.52      2370\n",
      "\n",
      "Accuracy on validation: 0.5189873417721519\n",
      "========================================================================================================================================================================================================\n",
      "Fold 4:\n",
      "Dataset: train; 0: 0.48; 1: 0.52\n",
      "Dataset: val; 0: 0.48; 1: 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.53      0.52      1149\n",
      "           1       0.54      0.52      0.53      1221\n",
      "\n",
      "    accuracy                           0.52      2370\n",
      "   macro avg       0.52      0.52      0.52      2370\n",
      "weighted avg       0.52      0.52      0.52      2370\n",
      "\n",
      "Accuracy on validation: 0.5227848101265823\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the X and y arrays\n",
    "X = data\n",
    "\n",
    "determine_label = lambda x: 0 if x == [1, 0] else 1\n",
    "y = [determine_label(i.y) for i in data]\n",
    "\n",
    "\n",
    "# Print the class ratio\n",
    "def print_class_ratio(y, ds_name=\"full\"):\n",
    "    l1 = sum(y) / len(y)\n",
    "    l0 = 1 - l1\n",
    "    print(f\"Dataset: {ds_name}; 0: {l0:.2f}; 1: {l1:.2f}\")\n",
    "\n",
    "\n",
    "print_class_ratio(y)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Use stratified k-fold cross validation to evaluate the model on the dataset\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    X_train = [i.x[0] for i in data[train_index]]\n",
    "    y_train = [determine_label(i.y) for i in data[train_index]]\n",
    "    print_class_ratio(y_train, ds_name=\"train\")\n",
    "\n",
    "    X_val = [i.x[0] for i in data[val_index]]\n",
    "    y_val = [determine_label(i.y) for i in data[val_index]]\n",
    "    print_class_ratio(y_val, ds_name=\"val\")\n",
    "\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    acc = clf.score(X_val, y_val)\n",
    "\n",
    "    y_hat = clf.predict(X_val)\n",
    "\n",
    "    report = classification_report(y_val, y_hat)\n",
    "    print(report)\n",
    "\n",
    "    rec = recall_score(y_val, y_hat)\n",
    "    prec = precision_score(y_val, y_hat)\n",
    "    f1 = f1_score(y_val, y_hat)\n",
    "\n",
    "    results.append((acc, rec, prec, f1))\n",
    "\n",
    "    print(f\"Accuracy on validation: {acc}\")\n",
    "\n",
    "    print(100 * \"==\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the experiment results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  dataset       acc       rec      prec        f1  method  fold\n0       p  0.522143  0.515548  0.538002  0.526536  LogReg     1\n1       p  0.547448  0.556921  0.561056  0.558981  LogReg     2\n2       p  0.530156  0.545455  0.543673  0.544563  LogReg     3\n3       p  0.518987  0.464373  0.538462  0.498681  LogReg     4\n4       p  0.522785  0.515971  0.538462  0.526976  LogReg     5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>acc</th>\n      <th>rec</th>\n      <th>prec</th>\n      <th>f1</th>\n      <th>method</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>p</td>\n      <td>0.522143</td>\n      <td>0.515548</td>\n      <td>0.538002</td>\n      <td>0.526536</td>\n      <td>LogReg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>p</td>\n      <td>0.547448</td>\n      <td>0.556921</td>\n      <td>0.561056</td>\n      <td>0.558981</td>\n      <td>LogReg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>p</td>\n      <td>0.530156</td>\n      <td>0.545455</td>\n      <td>0.543673</td>\n      <td>0.544563</td>\n      <td>LogReg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>p</td>\n      <td>0.518987</td>\n      <td>0.464373</td>\n      <td>0.538462</td>\n      <td>0.498681</td>\n      <td>LogReg</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>p</td>\n      <td>0.522785</td>\n      <td>0.515971</td>\n      <td>0.538462</td>\n      <td>0.526976</td>\n      <td>LogReg</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the results in a dataframe\n",
    "results_dic = {\"dataset\": dataset_id, \"acc\": [i[0] for i in results], \"rec\": [i[1] for i in results],\n",
    "               \"prec\": [i[2] for i in results], \"f1\": [i[3] for i in results], \"method\": \"LogReg\",\n",
    "               \"fold\": [i for i in range(1, 6)]}\n",
    "\n",
    "results_df = pd.DataFrame(results_dic)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Save the results into the clipboard\n",
    "results_df.transpose().to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
